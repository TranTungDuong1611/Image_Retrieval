{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "from bs4 import BeautifulSoup  # For parsing HTML content\n",
    "from urllib.parse import urljoin, urlparse  # For handling URLs\n",
    "import urllib.request  # For making HTTP requests\n",
    "import time  # For handling time-related operations\n",
    "import os  # For interacting with the operating system (relate to dir, folder, file)\n",
    "from tqdm import tqdm  # For displaying progress bars (visualize progress)\n",
    "import concurrent.futures  # For multi-threading\n",
    "import json  # For writing to a text file\n",
    "from PIL import Image  # For handling images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crawl image links from website (Brute force)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = \"https://www.flickr.com/search/?text=\"\n",
    "search_term = 'cat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_url_images(driver, max_images):\n",
    "    urls = [] # Store links of all images we want\n",
    "    more_content_available = True # Checking variable for manage lazy loading page\n",
    "\n",
    "    # Crawl url images until the number of link = max img we want or the page don't have any image\n",
    "    while len(urls) < max_images and more_content_available:\n",
    "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "        img_tags = soup.find_all(\"img\")\n",
    "\n",
    "        # Extract url image from HTML source\n",
    "        for img in img_tags:\n",
    "            if len(urls) >= max_images:\n",
    "                break\n",
    "            if 'src' in img.attrs:\n",
    "                href = img.attrs['src']\n",
    "                img_path = urljoin(url, href)\n",
    "                img_path = img_path.replace(\"_m.jpg\", \"_b.jpg\").replace(\"_n.jpg\", \"_b.jpg\").replace(\"_w.jpg\", \"_b.jpg\")\n",
    "                if img_path == \"https://combo.staticflickr.com/ap/build/images/getty/IStock_corporate_logo.svg\":\n",
    "                    continue\n",
    "                urls.append(img_path)\n",
    "\n",
    "\n",
    "        # Click load more button or scroll page for more image\n",
    "        try:\n",
    "            load_more_button = WebDriverWait(driver, 10).until(\n",
    "                EC.element_to_be_clickable((By.XPATH, '//button[@id=\"yui_3_16_0_1_1721642285931_28620\"]'))\n",
    "            )\n",
    "            load_more_button.click()\n",
    "            time.sleep(2) # Wait for generating content\n",
    "        except:\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            time.sleep(2) # Wait for generating content\n",
    "\n",
    "            # Check number of new generating image\n",
    "            new_soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "            new_img_tags = new_soup.find_all(\"img\", loading_=\"lazy\")\n",
    "            if len(new_img_tags) == len(img_tags):\n",
    "                more_content_available = False\n",
    "            img_tags = new_img_tags\n",
    "\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = URL + search_term\n",
    "max_images = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = webdriver.ChromeOptions()\n",
    "# options.add_argument('--headless')\n",
    "options.add_argument('--no-sandbox')\n",
    "options.add_argument('--disable-dev-shm-usage')\n",
    "driver = webdriver.Chrome(options=options)\n",
    "driver.get(url)\n",
    "\n",
    "urls = get_url_images(driver, max_images)\n",
    "print(f\"Number of images retrieved: {len(urls)}\")\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crawl image links from website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UrlScraper:\n",
    "    # Constructor\n",
    "    def __init__(self, url_template, max_images=50, max_workers=4):\n",
    "        self.url_template = url_template # Link crawl\n",
    "        self.max_images = max_images # Max images\n",
    "        self.max_workers = max_workers # Thread\n",
    "        self.setup_environment() # Call for set up environment\n",
    "\n",
    "    # Set up environment for selenium\n",
    "    def setup_environment(self):\n",
    "        os.environ['PATH'] += ':/usr/lib/chromium-browser/'\n",
    "        os.environ['PATH'] += ':/usr/lib/chromium-browser/chromedriver/'\n",
    "\n",
    "    def get_url_images(self, term):\n",
    "        # Initialize Chrome driver\n",
    "        options = webdriver.ChromeOptions()\n",
    "        options.add_argument('--headless')\n",
    "        options.add_argument('--no-sandbox')\n",
    "        options.add_argument('--disable-dev-shm-usage')\n",
    "        driver = webdriver.Chrome(options=options)\n",
    "\n",
    "        url = self.url_template.format(search_term=term)\n",
    "        driver.get(url)\n",
    "\n",
    "        # Start crawl urls of image like brute force - the same mechanism with this but add some feature\n",
    "        urls = []\n",
    "        more_content_available = True\n",
    "\n",
    "        pbar = tqdm(total=self.max_images, desc=f\"Fetching images for {term}\") # Set up for visualize progress\n",
    "\n",
    "        while len(urls) < self.max_images and more_content_available:\n",
    "            soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "            img_tags = soup.find_all(\"img\")\n",
    "\n",
    "            for img in img_tags:\n",
    "                if len(urls) >= self.max_images:\n",
    "                    break\n",
    "                if 'src' in img.attrs:\n",
    "                    href = img.attrs['src']\n",
    "                    img_path = urljoin(url, href)\n",
    "                    img_path = img_path.replace(\"_m.jpg\", \"_b.jpg\").replace(\"_n.jpg\", \"_b.jpg\").replace(\"_w.jpg\", \"_b.jpg\")\n",
    "                    if img_path == \"https://combo.staticflickr.com/ap/build/images/getty/IStock_corporate_logo.svg\":\n",
    "                        continue\n",
    "                    urls.append(img_path)\n",
    "                    pbar.update(1)\n",
    "\n",
    "            try:\n",
    "                load_more_button = WebDriverWait(driver, 10).until(\n",
    "                    EC.element_to_be_clickable((By.XPATH, '//button[@id=\"yui_3_16_0_1_1721642285931_28620\"]'))\n",
    "                )\n",
    "                load_more_button.click()\n",
    "                time.sleep(2)\n",
    "            except:\n",
    "                driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "                time.sleep(2)\n",
    "\n",
    "                new_soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "                new_img_tags = new_soup.find_all(\"img\", loading_=\"lazy\")\n",
    "                if len(new_img_tags) == len(img_tags):\n",
    "                    more_content_available = False\n",
    "                img_tags = new_img_tags\n",
    "\n",
    "        pbar.close()\n",
    "        driver.quit()\n",
    "        return urls\n",
    "\n",
    "    def scrape_urls(self, categories):\n",
    "        all_urls = {category: {} for category in categories}\n",
    "\n",
    "        # Handle multi-threading for efficent installation\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=self.max_workers) as executor:\n",
    "            future_to_term = {executor.submit(self.get_url_images, term): (category, term)\n",
    "                                for category, terms in categories.items() for term in terms}\n",
    "\n",
    "            for future in tqdm(concurrent.futures.as_completed(future_to_term), total=len(future_to_term), desc=\"Overall Progress\"):\n",
    "                category, term = future_to_term[future]\n",
    "                try:\n",
    "                    urls = future.result()\n",
    "                    all_urls[category][term] = urls\n",
    "                    print(f\"\\nNumber of images retrieved for {term}: {len(urls)}\")\n",
    "                except Exception as exc:\n",
    "                    print(f\"\\n{term} generated an exception: {exc}\")\n",
    "        return all_urls\n",
    "\n",
    "    def save_to_file(self, data, filename):\n",
    "        with open(filename, 'w') as file:\n",
    "            json.dump(data, file, indent=4)\n",
    "        print(f\"Data saved to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = {\n",
    "    \"animal\": [\"Monkey\", \"Elephant\", \"cows\", \"Cat\", \"Dog\", \"bear\", \"fox\", \"Civet\", \"Pangolins\", \"Rabbit\", \"Bats\", \"Whale\", \"Cock\", \"Owl\", \"flamingo\", \"Lizard\", \"Turtle\", \"Snake\", \"Frog\", \"Fish\", \"shrimp\", \"Crab\", \"Snail\", \"Coral\", \"Jellyfish\", \"Butterfly\", \"Flies\", \"Mosquito\", \"Ants\", \"Cockroaches\", \"Spider\", \"scorpion\", \"tiger\", \"bird\", \"horse\", \"pig\", \"Alligator\", \"Alpaca\", \"Anteater\", \"donkey\", \"Bee\", \"Buffalo\", \"Camel\", \"Caterpillar\", \"Cheetah\", \"Chicken\", \"Dragonfly\", \"Duck\", \"panda\", \"Giraffe\"],\n",
    "    \"plant\": [\"Bamboo\", \"Apple\", \"Apricot\", \"Banana\", \"Bean\", \"Wildflower\", \"Flower\", \"Mushroom\", \"Weed\", \"Fern\", \"Reed\", \"Shrub\", \"Moss\", \"Grass\", \"Palmtree\", \"Corn\", \"Tulip\", \"Rose\", \"Clove\", \"Dogwood\", \"Durian\", \"Ferns\", \"Fig\", \"Flax\", \"Frangipani\", \"Lantana\", \"Hibiscus\", \"Bougainvillea\", \"Pea\", \"OrchidTree\", \"RangoonCreeper\", \"Jackfruit\", \"Cottonplant\", \"Corneliantree\", \"Coffeeplant\", \"Coconut\", \"wheat\", \"watermelon\", \"radish\", \"carrot\"],\n",
    "    \"furniture\": [\"bed\", \"cabinet\", \"chair\", \"chests\", \"clock\", \"desks\", \"table\", \"Piano\", \"Bookcase\", \"Umbrella\", \"Clothes\", \"cart\", \"sofa\", \"ball\", \"spoon\", \"Bowl\", \"fridge\", \"pan\", \"book\"],\n",
    "    \"scenery\": [\"Cliff\", \"Bay\", \"Coast\", \"Mountains\", \"Forests\", \"Waterbodies\", \"Lake\", \"desert\", \"farmland\", \"river\", \"hedges\", \"plain\", \"sky\", \"cave\", \"cloud\", \"flowergarden\", \"glacier\", \"grassland\", \"horizon\", \"lighthouse\", \"plateau\", \"savannah\", \"valley\", \"volcano\", \"waterfall\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urltopic = {\"flickr\": \"https://www.flickr.com/search/?text={search_term}\"}\n",
    "scraper = UrlScraper(url_template=urltopic[\"flickr\"], max_images=20, max_workers=5)\n",
    "image_urls = scraper.scrape_urls(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraper.save_to_file(image_urls, 'image_urls.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download data from json file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDownloader:\n",
    "    def __init__(self, json_file, download_dir='Dataset', max_workers=4, delay=1):\n",
    "        self.json_file = json_file  # file containing URLs of images in JSON format\n",
    "        self.download_dir = download_dir  # Folder name for storing images\n",
    "        self.max_workers = max_workers  # Number of threads\n",
    "        self.delay = delay  # Polite delay: when we send request too much to the server for downloading images without polite delay, it will crash or prevent your IP\n",
    "        self.filename = set()  # To store filename directories\n",
    "        self.setup_directory()  # Set up the folder structure\n",
    "\n",
    "    def setup_directory(self):\n",
    "        if not os.path.exists(self.download_dir):\n",
    "            os.makedirs(self.download_dir)\n",
    "\n",
    "    def read_json(self):\n",
    "        with open(self.json_file, 'r') as file:\n",
    "            data = json.load(file)\n",
    "        return data\n",
    "\n",
    "    def is_valid_url(self, url):\n",
    "        try:\n",
    "            with urllib.request.urlopen(url) as response:\n",
    "                if response.status == 200 and 'image' in response.info().get_content_type():\n",
    "                    return True\n",
    "        except Exception:\n",
    "            return False\n",
    "\n",
    "    def download_image(self, url, category, term, pbar):\n",
    "        if not self.is_valid_url(url):\n",
    "            pbar.update(1)\n",
    "            return f\"Invalid URL: {url}\"\n",
    "\n",
    "        category_dir = os.path.join(self.download_dir, category)\n",
    "        if not os.path.exists(category_dir):\n",
    "            os.makedirs(category_dir)\n",
    "\n",
    "        term_dir = os.path.join(category_dir, term)\n",
    "        if not os.path.exists(term_dir):\n",
    "            os.makedirs(term_dir)\n",
    "\n",
    "        filename = os.path.join(term_dir, os.path.basename(urlparse(url).path))\n",
    "\n",
    "        self.filename.add(filename)  # Record the filename directory\n",
    "\n",
    "        try:\n",
    "            urllib.request.urlretrieve(url, filename)\n",
    "            pbar.update(1)\n",
    "            return f\"Downloaded: {url}\"\n",
    "        except Exception as e:\n",
    "            pbar.update(1)\n",
    "            return f\"Failed to download {url}: {str(e)}\"\n",
    "\n",
    "    def download_images(self):\n",
    "        data = self.read_json()\n",
    "        download_tasks = []\n",
    "\n",
    "        total_images = sum(len(urls) for terms in data.values() for urls in terms.values())\n",
    "        with tqdm(total=total_images, desc=\"Downloading images\") as pbar:\n",
    "            with concurrent.futures.ThreadPoolExecutor(max_workers=self.max_workers) as executor:\n",
    "                for category, terms in data.items():\n",
    "                    for term, urls in terms.items():\n",
    "                        for url in urls:\n",
    "                            download_tasks.append(executor.submit(self.download_image, url, category, term, pbar))\n",
    "                            time.sleep(self.delay)\n",
    "\n",
    "                for future in concurrent.futures.as_completed(download_tasks):\n",
    "                    print(future.result())\n",
    "\n",
    "        self.export_filename()\n",
    "\n",
    "    def export_filename(self):\n",
    "        with open('filename.txt', 'w') as file:\n",
    "            for filename in sorted(self.filename):\n",
    "                file.write(f\"{filename}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "downloader = ImageDownloader(json_file='image_urls.json', download_dir='Dataset', max_workers=4, delay=1)\n",
    "downloader.download_images()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "downloader.export_filename()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_and_preprocess_images(image_dir):\n",
    "    for root, _, files in os.walk(image_dir):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            try:\n",
    "                with Image.open(file_path) as img:\n",
    "                    # Check if image is smaller than 50x50 pixels\n",
    "                    if img.size[0] < 50 or img.size[1] < 50:\n",
    "                        os.remove(file_path)\n",
    "                        print(f\"Deleted {file_path}: Image too small ({img.size[0]}x{img.size[1]})\")\n",
    "                        continue\n",
    "\n",
    "                    # Convert non-RGB images to RGB\n",
    "                    if img.mode != 'RGB':\n",
    "                        img = img.convert('RGB')\n",
    "                        img.save(file_path)\n",
    "                        print(f\"Converted {file_path} to RGB\")\n",
    "\n",
    "            except Exception as e:\n",
    "                # If file is not an image, delete it\n",
    "                os.remove(file_path)\n",
    "                print(f\"Deleted {file_path}: Not an image or corrupted file ({str(e)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_and_preprocess_images('Dataset')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
